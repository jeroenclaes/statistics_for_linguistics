{
    "collab_server" : "",
    "contents" : "---\ntitle: 'Class 4: Comparing the central tendency of two groups'\nauthor: \"Jeroen Claes\"\ndate: \"26/2/2018\"\noutput:\n  revealjs::revealjs_presentation:\n    css: white.css\n    self_contained: no\n    transition: none\n  html_document:\n    toc: no\n  word_document:\n    toc: no\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE, eval=TRUE, message=FALSE, warning=FALSE, error=FALSE, fig.height = 4, dpi = 300)\nlibrary(pander)\nlibrary(dplyr)\n```\n\n## Contents\n1. Comparing the median with box-and-whisker plots\n2. The basis of inferential statistics: Hypothesis testing\n3. Comparing normally distributed data with t-tests\n4. A little side note on Standard Errors and confidence intervals\n5. Comparing non-normally distributed data with Wilcoxon-tests\n6. Comparing groups with plots\n7. Questions\n8. References\n\n## 1. Data for this class\n- 58 responses from a visual lexical decision latency experiment for beginning learners (8 year-old Dutch children) by Perdijk et al. (2006) (data provided by Baayen, 2013). \n- Here we will work with the responses to just two words: *grinniken* 'to chuckle' and *ui* ('onion'). \n- We will use a subset of three columns:\n    - Word \n    - Subject (participant code)\n    - LogRT (logarithm of the reaction times in the lexical decision experiment)\n \n```{r data}\nlibrary(readr)\ndataSet<-read_csv(\"http://www.jeroenclaes.be/statistics_for_linguistics/datasets/class4_Perdijk_et_al_2006.csv\")\nsummary(dataSet$LogRT)\n```\n\n## 1. Comparing the median with box-and-whisker plots\n## 1. Comparing the median with box-and-whisker plots\n- Box-and whisker plots are ideal for comparing the median of two groups\n- In the previous class, we put `1` on the x-axis and our variable on the y-axis\n- Here, we put our *grouping factor* (`Word`) on the x-axis and `LogRT` on the y-axis\n\n```{r box-whisker, eval=F}\nlibrary(ggplot2)\nggplot(dataSet, aes(x=Word, y=LogRT)) + geom_boxplot()\n```\n\n## 1. Comparing the median with box-and-whisker plots\n- The plot shows that the median response to *ui* was bit faster than the median response to *grinniken*\n\n```{r box-whisker1, eval=T}\nlibrary(ggplot2)\nggplot(dataSet, aes(x=Word, y=LogRT)) + geom_boxplot()\n```\n\n## 2. Inferential statistics and hypothesis testing\n## 2. Inferential statistics and hypothesis testing\n- Inferential statistics tries to infer a property of a population by studying a sample\n- Most often, we try to test a hypothesis about a property of a population by analyzing a sample\n\n## 2. Hypothesis testing (1/3)\n- Before collecting data it is important to have:\n    1. A research question: \n        - *What is the relationship between words and reaction times in an auditory lexical decision task?*\n     2. Hypotheses: (theoretically motivated) tentative answers to these research questions: \n        - *A short, common word like* ui *will be recognized faster than a longer, less common word like* grinniken\n\n## 2. Hypothesis testing (2/3)\n- Each hypothesis (called *alternative hypothesis*) has its *null hypothesis*:  \n    - *There is no difference words like* ui *and words like* grinniken \n- A hypothesis test tries to estimate the probability of obtaining the observed or more extreme results assuming the null hypothesis\n- This way, it tries to establish wether the difference between the groups \n    - is reliably large\n    - is larger than would be predicted by chance\n\n## 2. Hypothesis testing (3/3)\n- It is important to formulate your hypotheses very precisely:\n    - Directional hypothesis: ui *will be recognized faster than* grinniken:\n        - You need to perform a **one-tailed** test, which evaluates if the reaction time is shorter than would be expected under the null hypothesis\n    - Non-directional hypothesis: ui *and* grinniken *will have different reaction times*:\n        - You need to perform a **two-tailed** test, which evaluates if reaction times are longer **OR** shorter than would be expected under the null hypothesis\n \n## 2.1 The logic/process of hypothesis testing (1/9)\n- Suppose there are two groups who participated in the same experiment\n- Research question: \n    - *Did groupA do different from groupB in the experiment?*\n- Hypothesis:\n    - *groupB scored better than groupA*\n- Null hypothesis:\n    - *groupB did not score beter than groupA*\n \n## 2.1 The logic/process of hypothesis testing (2/9)\n```{r box-data, eval=TRUE, echo=FALSE, asis=TRUE, results='asis'}\nexperiment <- data.frame(groupA=c(1,2,3,4,5), groupB=c(6,7,8,9,10))\npandoc.table(experiment)\n```\n\n## 2.1 The logic/process of hypothesis testing (3/9)\n- To explore if the difference in the *means* of the two groups is reliably large:\n    - We calculate some statistic based on your sample data\n    - Here we use the `t-statistic` we will discuss later on\n    - This statistic belongs to a well-known distribution type\n \n```{r t-hand1, eval=TRUE, echo=TRUE}\n(mean(experiment$groupA)-mean(experiment$groupB))/\n sqrt((var(experiment$groupA)/nrow(experiment))+(var(experiment$groupB)/nrow(experiment)))\n``` \n\n## 2.1 The logic/process of hypothesis testing (4/9)\n- We determine our degrees of freedom: \n    - number of observations - number of groups\n```{r t-hand2, eval=TRUE, echo=TRUE}\ndf <- (nrow(experiment)*2) - ncol(experiment)\ndf\n``` \n\n## 2.1 The logic/process of hypothesis testing (5/9)\n- For each distribution type, there are tables that specify with which probability some value can occur in the distribution\n    -  For the normal distribution values close to the mean always have a very high probability of occurring by chance/under the null hypothesis\n- To go from the statistic to this probability, we compare our statistic (`r (mean(experiment$groupA)-mean(experiment$groupB))/sqrt((var(experiment$groupA)/nrow(experiment))+(var(experiment$groupB)/nrow(experiment)))`) to the row that contains the t-distribution for 8 degrees of freedom\n \n```{r t-hand3, eval=TRUE, echo=FALSE, results=\"asis\", asis=TRUE}\npandoc.table(as.data.frame(read_csv('df,\".40\",\".25\",\".10\",\".05\",\".025\",\".01\",\".005\",\".001\",\".0005\"\n4,0.271,0.741,1.533,2.132,2.776,3.747,4.604,7.173,8.610\n5,0.267,0.727,1.476,2.015,2.571,3.365,4.032,5.893,6.869\n6,0.265,0.718,1.440,1.943,2.447,3.143,3.707,5.208,5.959\n7,0.263,0.711,1.415,1.895,2.365,2.998,3.499,4.785,5.408\n8,0.262,0.706,1.397,1.860,2.306,2.896,3.355,4.501,5.041'))) \n``` \n \n## 2.1 The logic/process of hypothesis testing (6/9)\n- If your hypothesis is non-directional (`different from` rather than `lower` or `higher`), you multiply the probability by two (the value may occur in the left or the right tail of the distribution)\n- Our hypothesis is directional, so we conclude that the chance/probability that the null is true is less than 0.0005 (1/2000)\n\n## 2.1 The logic/process of hypothesis testing (7/9)\n- Now we check if that probability is lower than our pre-set **alpha-level**:\n    - Common alpha level: 0.05 (1/20)\n    - Expresses that we will accept the null hypothesis if there is a 5% probability (1/20 chance) that it is true\n- If the probability is lower than the alpha-level, we *reject* the null hypothesis \n- If the probability is higher, we are forced to accept the null hypothesis\n- Here we find that p < 0.05, so we reject the null hypothesis: \n    - groupA scored reliably worse than groupB\n\n## 2.1 The logic/process of hypothesis testing (8/9)\n- It is important to determine your alpha-level carefully:\n    - Too strict: too many *Type I errors* (false positives: we reject the null, but it is true)\n    - Too relaxed: too many *Type II errors* (false negatives: we accept the null, but it is false)\n- For most tests, the p-value will be smaller for larger samples. For large samples you should use lower alpha-levels (0.01 or 0.001)\n\n## 2.1 The logic/process of hypothesis testing (9/9)\n- Observe that our actual hypothesis was only tested indirectly, in two ways:\n - We do not test the hypothesis, but rather the null hypothesis\n - We do not test the null hypothesis, but rather the probability of obtaining the same or a more extreme statistic assuming the null hypothesis\n \n## 2.2 A note on unethical practices:\n- It is borderline unethical to come up with hypotheses after you have taken a look at your data\n- It is flatout unethical to tamper with your alpha-level once you have explored your results\n- p-values depend on sample size. A non-significant result may become significant with more observations. It is unethical to keep adding data until you reach p < 0.05 \n\n## 2.3 Some common misconceptions\n- There's no 'more' or 'less' significant. Either p < 0.05 or not\n- p-values have **nothing** to do with the size of a difference between groups. Differences are **not** stronger if p-values are lower!\n- p < 0.05 does **not** mean that your results are necesarily relevant\n- p > 0.05 does **not** mean that your results are irrelevant.\n- p < 0.05 does **not** mean that our hypothesis is true, or that our null hypothesis is false. p < 0.05 suggests that there is less than a 1/20 chance of finding data that supports the null hypothesis\n\n## 3. Comparing normally distributed data with t-tests\n## 3. Comparing normally distributed data with t-tests\n- The test we used to compare the two groups was a t-test\n- This test can be used to compare the means of two groups for normally distributed data\n- Two types:\n    - paired/dependent:\n        - Each observation in one group has a related observation in the other group. E.g. Experiment in which participants perform a task, undergo some experimental condition, and then perform the same task again\n    - unpaired/independent:\n        - The observations in the two groups are completely independent. E.g., Experiment with test and control groups\n\n## 3.1 Unpaired t-test\n- The paired t-test has some important assumptions (Levshina, 2015: 88):\n    - The samples have been randomly selected from the populations they represent \n    - The observations are independent, both between the groups and within the groups. \n    - The variable is quantitative or at least ordinal (e.g., Likert-scales)\n    - The data in **both** samples are normally distributed, and/or the sample sizes are greater than 30 \n    - The variances of the samples should be equal (less important, the `t.test` implementation in R corrects for that)\n \n## 3.2 Performing an unpaired t-test in R (1/4)\n- Both versions of the t-test can be performed with the same function in R\n- Here we perform a t-test to see if *ui* is recognized faster than *grinniken*\n- Hypothesis:\n    - *short, common words like *ui* are recognized faster in a lexical decision experiment than longer, less common words like * grinniken\n- Null hypothesis:\n    - *short, common words like *ui* are not recognized faster in a lexical decision experiment than longer, less common words like * grinniken\n \n## 3.2 Performing an unpaired t-test in R (2/4)\n- Our data approaches a normal distribution for both samples\n- With `r nrow(dataSet)` observations, we also have enough data points\n```{r t-machine0, eval=TRUE, echo=TRUE}\nshapiro.test(dataSet[dataSet$Word==\"ui\",]$LogRT)\nshapiro.test(dataSet[dataSet$Word==\"grinniken\",]$LogRT)\n```\n\n## 3.2 Performing an unpaired t-test in R (3/4)\n- `t.test` accepts multiple columns as arguments\n- The `paired` argument specifies that it is an unpaired t-test. \n- The `alternative` argument specifies that it is a one-tailed t-test. \n    - The alternative hypothesis is that *ui* will have a smaller mean LogRT than *grinniken* (i.e., `mean(ui) - mean(grinniken) < 0`), so we specify `less`\n - Other values are:\n    - `two.sided`: two-tailed test\n    - `greater`: alternative hypothesis states that the mean of the second group is higher than the mean of the first group\n- Be careful to use the right `alternative` setting\n- Be careful to order the arguments the right way:\n    - The column that is hypothesized to have lower/higher scores must come first!\n```{r t-machine1, eval=TRUE, echo=TRUE}\nui <- dataSet[dataSet$Word ==\"ui\",]\ngrinniken <- dataSet[dataSet$Word ==\"grinniken\",]\nt.test( ui$LogRT, grinniken$LogRT,alternative=\"less\", paired = FALSE)\n```\n\n## 3.2 Performing an unpaired t-test in R (4/4)\n- `t.test` also accepts a formula specification and a data argument\n- Make sure that your grouping factor has the right order: \n    - The group that is hypothesized to have lower/higher scores must be the reference level\n```{r t-machine2, eval=TRUE, echo=TRUE}\ndataSet$Word <- as.factor(dataSet$Word)\ndataSet$Word <- relevel(dataSet$Word, ref=\"ui\")\nt.test(LogRT ~ Word, data=dataSet, alternative=\"less\", paired = FALSE)\n```\n\n## 3.3 Paired t-test\n- The paired t-test has some important assumptions (Levshina, 2015: 88):\n  - The **subjects** have been sampled randomly\n  - The variable is quantitative \n  - The differences between the pairs of scores (not the scores themselves!) are normally distributed, and/or the sample size is greater than 30\n\n```{r t-paired0, eval=TRUE, echo=FALSE, results=\"asis\"}\n\ndataSet2 <- dataSet %>% \n group_by(Subject) %>% \n filter(n() >1) %>%\n mutate(diff=LogRT[which(Word==\"grinniken\")]-LogRT[which(Word==\"ui\")]) %>%\n ungroup() %>%\n  arrange(Subject)\n```\n\n## 3.4 Performing a paired t-test in R (1/3)\n- Some data-wrangling later we have a nice paired dataset of `r nrow(dataSet2)` observations based on our dataset:\n    - Reaction times for all the subjects for the two words\n```{r t-paired, eval=TRUE, echo=FALSE, results=\"asis\"}\npandoc.table(head(dataSet2, 10))\n```\n\n## 3.4 Performing a paired t-test in R (2/3)\n- The sample size is greater than 30 (`r nrow(dataSet2)` rows)\n- The differences between our scores are normally distributed\n```{r t-paired1, eval=TRUE, echo=TRUE, fig.height=3}\nshapiro.test(dataSet2[!duplicated(dataSet2$diff), ]$diff)\nqqnorm(dataSet2[!duplicated(dataSet2$diff), ]$diff)\nqqline(dataSet2[!duplicated(dataSet2$diff), ]$diff)\n```\n\n## 3.4 Performing a paired t-test in R (3/3)\n- By setting the argument `paired=TRUE` we tell R that we want a paired t-test\n- Hypothesis: \n    - *Subjects will behave differently for the two words*\n- Null hypothesis:\n    - *There will be no difference between the two words*\n \n```{r t-paired2, eval=TRUE, echo=TRUE}\ndataSet2$Word <- as.factor(dataSet2$Word)\ndataSet2$Word <- relevel(dataSet2$Word, ref=\"ui\")\nt.test(LogRT ~ Word, data=dataSet2, alternative=\"two.sided\")\n\n```\n\n## 3.5 Interpreting a t-test output (1/4)\n- Let's take a minute to interpret the rich output of `t.test`:\n    - Test statistic `t`\n    - `df`= degrees of freedom\n    - `p-value`\n    - Confidence interval:\n        - If we were to repeat the same experiment over and over again with different samples, there would be a 95% probability that the interval contains the differences between the means of all these experiments \n - If your confidence interval includes `0` that's a bad thing\n\n## 3.5 Interpreting a t-test output (2/4)\n- Observe that the t-test only provides information on whether or not the difference between the means is significant\n- It tells us nothing about the size or importance of the difference\n- You should always include a measure of effect size:\n    - If you have only one variable, the difference between the means (or a standardized measure of effect size)\n    - If you have multiple variables, a standardized measure of effect size\n \n## 3.5 Interpreting a t-test output: effect size (3/4)\n- For differences between means,`Cohen's d` is the effect size measure of choice:\n    - `Mean 1 - Mean 2 / standard deviation`\n    - Expresses the difference between the means in standard deviation units\n - Here we find that the mean `LogRT` of  *ui* is 0.47 standard deviation units smaller than the mean `LogRT` of *grinniken*\n\n```{r effectSize, eval=TRUE, echo=TRUE}\nd <- (mean(dataSet[dataSet$Word==\"ui\",]$LogRT) - mean(dataSet[dataSet$Word==\"grinniken\",]$LogRT))/\n sd(dataSet$LogRT)\nd\n``` \n\n## 3.5 Interpreting a t-test output (4/4)\n- When you report differences between means, you report:\n    - The two means\n    - Which test you used (paired, unpaired, two-tailed, one-tailed)\n    - T-statistic value\n    - Degrees of freedom\n    - p-value (rounded: e.g., not p = 00001244, but rather: p < 0.05)\n    - If you are comparing multiple groups in a single text, a standardized effect size measure\n    - The standard error of the mean or the confidence interval of the mean\n\n## 4. A little side note on Standard Errors and confidence intervals\n## 4.1 Standard Error (1/3)\n- Standard errors are an abstract and difficult concept to grasp\n- Suppose the following:\n    - Our `population` consists of all numbers between one and ten\n    - We sample 100 x 100 random values from this population, allowing the same value to be included more than once\n    - We calculate the `mean` for these 100 samples\n    - We get a new distribution of slightly different mean values. This is called the `sampling distribution of the mean`\n \n```{r samplingDistribution, eval=TRUE, echo=TRUE}\npopulation <- c(1:10)\nmeans <- sapply(1:100, FUN=function(x) {\n sample <- sample(population, 100, replace=TRUE)\n return(mean(sample))\n})\nmeans\n``` \n\n## 4.1 Standard Error (2/3)\n- If we take the standard deviation of the sampling distribution of the mean of a population, we obtain the `standard error` (SE) of the mean\n- The standard error is a hugely important statistic:\n    - It is the denominator in many statistical tests. E.g. the t-test without variance correction is defined as:\n    - `(Mean 1 - Mean 2)/SE`\n```{r samplingDistribution2, eval=TRUE, echo=TRUE}\nSE <- sd(means)\nSE\n``` \n\n## 4.1 Standard Error (3/3)\n- Luckily, statisticians have come up with a way to estimate the standard error (i.e., the standard deviation of the sampling distribution of the mean) from a single sample:\n    - `standard deviation/square root of sample size`\n```{r samplingDistribution3, eval=TRUE, echo=TRUE}\nsample <- sample(population, 100, replace=TRUE)\nSE <- sd(sample)/sqrt(length(sample))\nSE\n``` \n\n## 4.2 Confidence intervals (1/3)\n- With the standard error of the mean we can also calculate confidence intervals for the means of two groups and their difference\n    - Confidence interval:\n    - If we were repeat the same experiment over and over again with different samples, there would be a 95% probability that the interval contains the values of all these experiments \n \n## 4.2 Confidence intervals (2/3)\n- We multiply the standard error with `1.96`, the t-value for `Infinite degrees of freedom` and p = 0.05 (1 - 0.05 = 0.95)\n- The lower bound is defined as the mean *minus* this value, the upper bound as the mean *plus* this value\n- Confidence interval for the mean:\n    - Upper bound: `mean + (1.96*SE)`\n    - Lower bound: `mean - (1.96*SE)`\n```{r CONFINT, eval=TRUE, echo=TRUE}\nmeanUi <- mean(dataSet[dataSet$Word==\"ui\",]$LogRT)\nmeanGrinniken <- mean(dataSet[dataSet$Word==\"grinniken\",]$LogRT)\nSE <- sd(dataSet$LogRT)/sqrt(nrow(dataSet))\n\nconfmeanUi <- c(meanUi-(1.96*SE), meanUi+(1.96*SE))\nconfmeanUi \n\nconfmeanGrinniken <- c(meanGrinniken-(1.96*SE), meanGrinniken+(1.96*SE))\nconfmeanGrinniken\n``` \n\n## 4.3 Confidence interval for the difference of the means and Cohen's d \n- We can use the same principle to calculate confidence intervals for the differences and the effect sizes\n```{r CONFINT2, eval=TRUE, echo=TRUE}\ndifference <- meanUi -meanGrinniken\nconfDifference <- c(difference-(1.96*SE), difference+(1.96*SE))\nconfDifference\n\neffectSize<-(meanUi -meanGrinniken)/sd(dataSet$LogRT)\nconfEffect <- c(effectSize-(1.96*SE), effectSize+(1.96*SE))\nconfEffect\n``` \n\n\n## 5. Comparing non-normally distributed data with Wilcoxon-tests\n\n## 5.1 Data\n - 3,381 frequent nouns and adjectives from the *Corpus of Contemporary North American English* (COCA), adapted from http://www.wordfrequency.info (Davies, 2018)\n    - word\n    - pos (part of speech)\n    - coca_frequency\n    - word_length\n \n```{r coca, eval=TRUE, echo=TRUE}\ndataSet2 <- read_csv(\"http://www.jeroenclaes.be/statistics_for_linguistics/datasets/class4_Davies_2008_coca_frequency.csv\")\nsummary(dataSet2$coca_frequency)\n``` \n\n## 5.2 Research design\n- Research question:\n    - *Are frequent nouns less frequent than frequent adjectives?*\n- Hypothesis:\n    - *Frequent nouns occur less often than frequent adjectives*\n- Null hypothesis:\n    - *Frequent nouns do not occur less often than frequent adjectives*\n \n## 5.3 When to use Wilcoxon tests\n- If your data is very different from a normal distribution, it is best not to use a t-test, even when your sample is large enough \n- A **Wilcoxon test** does not make any assumptions about the shape of the data\n- It tests differences in medians: \n    - What is the relative position of the median value of group A in the values of group B?  \n    - The null hypothesis is that the median does not shift left or right between the two groups (i.e., that its rank position does not change)\n- We can use the Wilcoxon test, if the data satisfies the following conditions (Levshina, 2015: 110):\n    - Each sample has been drawn randomly from the population\n    - The observations are independent, both between the two samples and within each sample\n    - The variable is quantitative\n    - The underlying population distributions should be of a similar shape\n\n## 5.3 When to use Wilcoxon tests\n```{r cocanormal, eval=TRUE, echo=TRUE}\nggplot(dataSet2[dataSet2$pos==\"noun\",], aes(x=coca_frequency)) + \n    geom_line(stat=\"density\") + \n    geom_vline(aes(xintercept=mean(dataSet2$coca_frequency)), color=\"red\") + labs(title=\"Nouns\")\n``` \n\n## 5.3 When to use Wilcoxon tests\n```{r cocanormal2, eval=TRUE, echo=TRUE}\nggplot(dataSet2[dataSet2$pos==\"adjective\",], aes(x=coca_frequency)) + \n    geom_line(stat=\"density\") + \n    geom_vline(aes(xintercept=mean(dataSet2$coca_frequency)), color=\"red\") + labs(title=\"Adjectives\")\n``` \n\n\n## 5.4 Computing a Wilcoxon test in R\n- Like the t-test, the Wilcoxon test accepts two numeric vectors or a formula with data\n- Like the t-test, the Wilcoxon test requires you to specify your alternative hypothesis:\n    - `two.tailed` for 'different'\n    - `less` if the median of the second group is hypothesized to be smaller\n    - `greater` if the median of the second group is hypothesized to be larger\n- If you use the Wilcoxon test for ordinal data (e.g., likert-scales), you have to set `correct=TRUE`\n- For use on paired data, you have to set `paired=TRUE`\n- `conf.int=TRUE` will give you a confidence interval of the difference between the medians of the two groups\n- `conf.level` defaults to 0.95 (95% confidence intervals), but you can set it to higher values if needed\n\n```{r wilcoxon, eval=TRUE, echo=TRUE}\ndataSet2$pos <- as.factor(dataSet2$pos)\ndataSet2$pos <- relevel(dataSet2$pos, ref=\"noun\")\nwilcox.test(coca_frequency ~ pos, data = dataSet2, alternative=\"greater\", conf.int=TRUE)\n``` \n\n## 5.5 Interpreting the output of a Wilcoxon test \n- The test suggests that nouns occur reliably less often than adjectives\n- Unfortunately, standardized effect size measures are not readily available for describing the effect size of non-normally distributed data. \n- Cohen's d should not be used here, as it assumes normally distributed data\n\n## 5.6 Confidence intervals for the median\n- We cannot use the standard error to compute confidence intervals here\n- The following (more advanced) code will compute the rank orders of the upper/lower limits of the confidence interval:\n    - We can turn it into a function to compute the confidence intervals of the median\n```{r confintMedian, eval=TRUE, echo=TRUE}\n\nconfMedian <- function(x) {\n  sort(x)[qbinom(c(.025,.975), length(x), 0.5)]\n}\nmedian(dataSet2$coca_frequency)\nconfMedian(dataSet2$coca_frequency)\n``` \n\n## 5.7 Interpreting the output of a Wilcoxon test\n- When you report on a Wilcoxon test you provide:\n    - Medians of the two groups\n    - Difference of the medians\n    - Confidence interval of the difference of the medians\n    - W-statistic\n    - p-value\n \n## 6. Comparing groups with plots\n## 6. Comparing groups with plots (1/3)\n- When you report a t-test, you will want to provide a barplot of the *means* \n- When you report a Wilcoxon test, you will want to provide a barplot of the *medians*\n- You can compute by-group means and confidence intervals easily with `dplyr`\n\n## 6. Comparing means with plots (2/3)\n- Consider the following code:\n    - `%>%`is a piping function: it passes the output of the previous function on to the first argument of the next function\n    - `group_by`: compute everything by `Word` group\n    - `summarise`: summarise the values into one line per group with several columns for each group\n\n```{r dplyr, eval=TRUE, echo=TRUE}\n# Standard error \nSE <- sd(dataSet$LogRT) /sqrt(nrow(dataSet))\n# Summarization of values\ndataForPLot <- dataSet %>%\n    group_by(Word) %>%\n    summarise( lowerBound=mean(LogRT) - (1.96*SE), \n    upperBound=mean(LogRT)+(1.96*SE), \n    mean=mean(LogRT))\ndataForPLot\n``` \n\n## 6. Comparing means with plots (3/3)\n- Now we can plot `dataForPlot` with `ggplot2`\n```{r plot, eval=TRUE, echo=TRUE}\nggplot(dataForPLot, aes(x=Word, y=mean, color=Word, fill=Word)) + geom_bar(stat=\"identity\") + geom_errorbar(aes(ymin=lowerBound, ymax=upperBound), color=\"black\", width=0.5) \n``` \n\n## 6. Comparing medians with plots (1/2)\n- If we pass our confidence interval function to `dplyr`, it will calculate the confidence intervals for the median of each group \n```{r dplyr2, eval=TRUE, echo=TRUE}\n# Confidence intervals for the median\nconfMedian <- function(x) {\n  sort(x)[qbinom(c(.025,.975), length(x), 0.5)]\n}\n\n\n# Summarization of values\ndataForPLot <- dataSet2 %>%\n  group_by(pos) %>%\n  summarise(lowerBound=confMedian(coca_frequency)[1], \n  upperBound=confMedian(coca_frequency)[2], \n  median=median(coca_frequency))\ndataForPLot\n``` \n\n## 6. Comparing medians with plots (2/2)\n- Now we can plot `dataForPlot` data with `ggplot2`\n```{r plotmedian, eval=TRUE, echo=TRUE}\nggplot(dataForPLot, aes(x=pos, y=median, color=pos, fill=pos)) + geom_bar(stat=\"identity\") + geom_errorbar(aes(ymin=lowerBound, ymax=upperBound), color=\"black\", width=0.5) \n``` \n\n## 7. Exercises\n- Go to http://www.jeroenclaes.be/statistics_for_linguistics/labs/class4.html and perform the exercises\n\n## 8. References\n- Davies, M. (2018). *Top-5000 most frequent words in Corpus of Contemporary American English (COCA)*. http://www.wordfrequency.info. \n- Baayen, R. H. (2013). *languageR: Data sets and functions with \"Analyzing Linguistic Data: A practical introduction to statistics\"*. https://cran.r-project.org/web/packages/languageR/index.html. \n- Levshina, N. (2015). *How to do Linguistics with R: Data exploration and statistical analysis*. Amsterdam/Philadelphia, PA: John Benjamins.\n- Perdijk, K., Schreuder, R., Verhoeven, L. and Baayen, R. H. (2006) Tracing individual differences in reading skills of young children with linear mixed-effects models. Manuscript, Radboud University Nijmegen.",
    "created" : 1519325295722.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1984425341",
    "id" : "87A2834F",
    "lastKnownWriteTime" : 1519492142,
    "last_content_update" : 1519492142640,
    "path" : "~/Desktop/statistics_for_linguistics/slides/Class4.Rmd",
    "project_path" : null,
    "properties" : {
        "last_setup_crc32" : "D61F5DEF4624882e",
        "tempName" : "Untitled2"
    },
    "relative_order" : 11,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}